
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8123341a-0bd6-4113-9063-04f6962fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning tutorial for three-level population transfer\n",
    "# Copyright (C) 2022 Luigi Giannelli\n",
    "\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7941d328-50b4-4019-870c-1f3344653c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from qutip import basis, expect, ket2dm, mesolve\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.random.set_seed(12357111317)\n",
    "# tf.random.set_seed(123571113171923)\n",
    "from ThreeLS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e409177-84d4-45d3-b1f4-b20c46ccb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce28e83-c592-4623-9646-c2614588e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if you want to force the not use of the GPU\n",
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b2857-739f-4adf-8334-11957d4970df",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d05ecc9-e393-4a5e-95bf-54ada44e4456",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hhm-5R7spVx"
   },
   "outputs": [],
   "source": [
    "Ωmax = 20\n",
    "n_steps = 30\n",
    "γ = 5\n",
    "T = 1\n",
    "reward_gain = 1.0\n",
    "\n",
    "fc_layer_params = (100, 50, 30)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_iterations = 1000\n",
    "collect_episodes_per_iteration = 2\n",
    "eval_interval = 10\n",
    "replay_buffer_capacity = 7 * n_steps\n",
    "\n",
    "env_parameters = {\n",
    "    \"Ωmax\": Ωmax,\n",
    "    \"n_steps\": n_steps,\n",
    "    \"γ\": γ,\n",
    "    \"T\": T,\n",
    "}\n",
    "\n",
    "env_train_py = ThreeLS_v0_env(Ωmax=Ωmax,\n",
    "                              n_steps=n_steps,\n",
    "                              γ=γ,\n",
    "                              T=T,\n",
    "                              reward_gain=1.0)\n",
    "env_eval_py = ThreeLS_v0_env(Ωmax=Ωmax,\n",
    "                             n_steps=n_steps,\n",
    "                             γ=γ,\n",
    "                             T=T,\n",
    "                             reward_gain=1.0)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env_train_py)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env_eval_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d633bf-b4ea-40fe-a5ac-aea5f263d3b7",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ffa44e-3ede-46a3-8371-8050f11eb17d",
   "metadata": {
    "id": "BwY7StuMkuV4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 20:23:46.879835: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# define the network that approximates the policy\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()  # (learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "# the agent\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter,\n",
    ")\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326a502-5368-4480-806d-dbe9dba030c2",
   "metadata": {
    "id": "NLva6g2jdWgr",
    "tags": []
   },
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4062ff3d-f717-4238-93a4-ff9f19a5834a",
   "metadata": {
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "\n",
    "eval_replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=eval_env.batch_size,\n",
    "    max_length=n_steps + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10262da9-4965-4921-bcb6-ac0c9ebc3144",
   "metadata": {
    "id": "rVD5nQ9ZGo8_",
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1429fb21-7412-4ff0-9eee-1c11af82f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "eval_observers = [avg_return, eval_replay_buffer.add_batch]\n",
    "eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(eval_env,\n",
    "                                                          eval_policy,\n",
    "                                                          eval_observers,\n",
    "                                                          num_episodes=1)\n",
    "\n",
    "train_observers = [replay_buffer.add_batch]\n",
    "train_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    train_env,\n",
    "    collect_policy,\n",
    "    train_observers,\n",
    "    num_episodes=collect_episodes_per_iteration,\n",
    ")\n",
    "\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "#tf_agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a162f-1595-46d3-a5bc-dae5062ebb93",
   "metadata": {
    "id": "hBc9lj9VWWtZ",
    "tags": []
   },
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052d50bc-f4b8-4f93-aff2-83c8c508bac1",
   "metadata": {
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 20:23:55.153044: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Average Return:  0.14341089\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent's policy once before training.\n",
    "final_time_step, policy_state = eval_driver.run()\n",
    "print(\"Initial Average Return: \", avg_return.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b9195f-8263-496d-81e2-dfece7995481",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list = []\n",
    "episode_list = []\n",
    "iteration_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39eb5b6f-7e69-43e2-937a-13b7b1a29a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweak num_iterations and eval_interval if you want to keep training the agent\n",
    "# num_iterations = 100\n",
    "# eval_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f58993f-6949-481b-8b32-21c02b3296c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb15158b77f450092b52fb32181661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luigi/work/projects/ML_STIRAP/numerics/scipy/threeLS_populationtransfer/src/ThreeLS.py:257: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luigi/work/projects/ML_STIRAP/numerics/scipy/threeLS_populationtransfer/src/ThreeLS.py:257: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luigi/.pyenv/versions/3.9.7/envs/threeLS/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luigi/.pyenv/versions/3.9.7/envs/threeLS/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:345: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    }
   ],
   "source": [
    "return_list_, episode_list_, iteration_list_ = run_training(\n",
    "    tf_agent,\n",
    "    train_driver,\n",
    "    replay_buffer,\n",
    "    eval_driver,\n",
    "    eval_replay_buffer,\n",
    "    avg_return,\n",
    "    num_iterations=num_iterations,\n",
    "    eval_interval=eval_interval,\n",
    "    save_episodes=True,\n",
    "    clear_buffer=False,\n",
    ")\n",
    "\n",
    "return_list += return_list_\n",
    "episode_list += episode_list_\n",
    "iteration_list += iteration_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc94f8e-8044-4698-bb37-66d787c42542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object([env_parameters, return_list, episode_list, iteration_list],\n",
    "#             \"./data/gamma5_Omax20_RL.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7401024-10a8-403b-af05-a00976161688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_parameters, return_list, episode_list, iteration_list = load_object(\n",
    "#     \"./data/gamma5_Omax20_RL.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90652b87-24ba-4e32-be1a-7ed79600759c",
   "metadata": {
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5baf8134-d145-459b-a762-223b3a7bdd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4754e4c1f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},